# DIABLO: Benchmarks

The DIABLO benchmarks were evaluated on [SDSC Comet](https://portal.xsede.org/sdsc-comet).
The SBATCH shell scripts used to run the benchmarks are comet1.run, comet2.run, and comet3.run.
The log files generated by the scripts that contain the run times are run-1.log, run-2.log, and run-3.log, respectively.

The cluster should support Slurm Workload Manager, Hadoop 2.6, and myhadoop.

The DIQL and benchmark binaries are already provided (lib/diql-spark.jar and benchmarks/diablo/test.jar).
You may recompile DIQL using `mvn install` on the top directory and compile the test files using `./compile` on the benchmarks/diablo directory.

Steps to run the scripts on Comet (or on any Slurm-managed cluster):

1. Install [Scala 2.11](https://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz).
2. Install [Spark 2.2 on Hadoop 2.6](https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.6.tgz).
3. Change SCALA_HOME and SPARK_HOME in the SBATCH scripts to point to your installations.
4. Execute the scripts using sbatch, eg, `sbatch comet1.run`.

The benchmarks that compare DIABLO on parallel Scala vs Scala lists are in the directory: benchmarks/diablo/parallel.
You compile with `mvn -f pom-parallel.xml install` and `mvn -f pom-sequential.xml install` on the top directory and
`./build-parallel` and `./build-sequential` on this directory. 
Use `sbatch comet-run` to run these benchmarks on one Comet node. The log file from the
evaluations is run.log.
