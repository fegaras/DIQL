#!/bin/bash
#SBATCH -A uot143
#SBATCH --job-name="diablo"
#SBATCH --output="comet-build.log"
#SBATCH --partition=shared
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --export=ALL
#SBATCH --time=20

# location of scala and spark
SW=/oasis/projects/nsf/uot143/$USER

DIQL_HOME=$SW/diql

if [ -z ${SCALA_HOME} ]; then
   SCALA_HOME=$SW/scala-2.11.8
fi

if [ -z ${SPARK_HOME} ]; then
   SPARK_HOME=$SW/spark-2.2.0-bin-hadoop2.6
fi

JARS=.
for I in ${SPARK_HOME}/jars/*.jar; do
    JARS=${JARS}:$I
done

rm -rf classes
mkdir -p classes

java_files=`ls casper/*.java`
for f in $java_files; do
    echo compiling $f ...
    javac -d classes -cp ${JARS} $f
done

scala_files=`ls *.scala`
for f in $scala_files; do
    echo compiling $f ...
    $SCALA_HOME/bin/scalac -d classes -cp classes:${JARS}:${DIQL_HOME}/lib/diql-spark.jar $f
done

jar cf test.jar -C classes .
